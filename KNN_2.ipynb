{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
        "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
        "\n",
        "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
        "used to determine the optimal k value?\n",
        "\n",
        "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
        "what situations might you choose one distance metric over the other?\n",
        "\n",
        "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
        "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
        "model performance?\n",
        "\n",
        "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
        "techniques can be used to optimize the size of the training set?\n",
        "\n",
        "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
        "overcome these drawbacks to improve the performance of the model?"
      ],
      "metadata": {
        "id": "jZHwLjq49xp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. The main difference between the Euclidean distance metric and the Manhattan distance metric in KNN is in how they measure the distance between data points:\n",
        "\n",
        "Euclidean Distance: Measures the straight-line (shortest) distance between two points in Euclidean space, considering the squares of differences along each dimension. It is sensitive to differences along all axes and can be affected by outliers.\n",
        "Manhattan Distance: Measures the distance as the sum of absolute differences between corresponding coordinates of the two points, considering only the absolute differences along each dimension. It is less sensitive to outliers and differences along individual dimensions.\n",
        "The choice of distance metric can significantly affect KNN's performance. If the dataset contains outliers or if certain dimensions are more important than others, Manhattan distance might be a better choice. Euclidean distance is suitable when all dimensions contribute equally and outliers are not a major concern.\n",
        "\n",
        "Q2. Choosing the optimal value of k in KNN involves experimentation and can be determined using techniques like cross-validation or grid search:\n",
        "\n",
        "Cross-Validation: Split the dataset into training and validation sets. Test the model's performance for different values of k and select the k that yields the best performance on the validation set.\n",
        "Grid Search: Systematically test multiple k values using cross-validation and select the k with the best performance.\n",
        "Q3. The choice of distance metric can significantly affect KNN's performance:\n",
        "\n",
        "Euclidean Distance: Works well when data follows a normal distribution and features have similar scales.\n",
        "Manhattan Distance: Works well when features are on different scales, and the dataset may contain outliers.\n",
        "The choice depends on the characteristics of the data and the problem. You might choose one distance metric over the other based on the nature of the data and the importance of different dimensions.\n",
        "\n",
        "Q4. Common hyperparameters in KNN classifiers and regressors include k, the distance metric, and sometimes weights (giving different weights to different neighbors). These hyperparameters can affect model performance:\n",
        "\n",
        "k: Affects the trade-off between bias and variance. Smaller k values can lead to high variance and sensitivity to noise, while larger k values can lead to high bias.\n",
        "Distance Metric: Influences how the algorithm measures similarity.\n",
        "Weights: Assign different weights to neighbors based on their distance.\n",
        "Tuning these hyperparameters can be done using techniques like cross-validation or grid search, as mentioned in the answer to Q2.\n",
        "\n",
        "Q5. The size of the training set can affect KNN's performance. Generally, larger training sets can lead to better generalization, but there's a trade-off with computational complexity. Techniques to optimize the size of the training set include:\n",
        "\n",
        "Cross-Validation: Use cross-validation to assess model performance with different training set sizes and choose an appropriate size that balances performance and efficiency.\n",
        "Sampling Methods: Use techniques like random sampling or stratified sampling to select a representative subset of the data for training.\n",
        "Q6. Potential drawbacks of using KNN as a classifier or regressor include:\n",
        "\n",
        "Sensitivity to the choice of hyperparameters, especially k.\n",
        "Computationally expensive for large datasets.\n",
        "Sensitive to irrelevant features.\n",
        "Inability to handle imbalanced datasets.\n",
        "To overcome these drawbacks, you can perform hyperparameter tuning, use dimensionality reduction techniques, preprocess data to remove irrelevant features, and employ techniques like resampling or weighted KNN to address class imbalance."
      ],
      "metadata": {
        "id": "BWtGwjhP92Bm"
      }
    }
  ]
}