{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
        "represent?\n",
        "\n",
        "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
        "\n",
        "Q3. When is it more appropriate to use adjusted R-squared?\n",
        "\n",
        "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
        "calculated, and what do they represent?\n",
        "\n",
        "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
        "regression analysis.\n",
        "\n",
        "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
        "it more appropriate to use?\n",
        "\n",
        "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
        "example to illustrate.\n",
        "\n",
        "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
        "choice for regression analysis.\n",
        "\n",
        "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
        "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
        "performer, and why? Are there any limitations to your choice of metric?\n",
        "\n",
        "Q10. You are comparing the performance of two regularized linear models using different types of\n",
        "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
        "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
        "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
        "method?"
      ],
      "metadata": {
        "id": "Vy1x2y6DOFrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. R-squared in Linear Regression:\n",
        "\n",
        "R-squared, often denoted as R², is a statistical metric used to measure the goodness of fit of a linear regression model. It represents the proportion of the variance in the dependent variable (target) that is explained by the independent variables (features) in the model.\n",
        "R-squared is calculated as the ratio of the explained variance to the total variance in the data. It ranges from 0 to 1, where 0 indicates that the model explains none of the variance, and 1 indicates that the model explains all of the variance.\n",
        "Mathematically, R-squared is calculated as: R² = 1 - (SSR/SST), where SSR is the sum of squared residuals (errors) and SST is the total sum of squares.\n",
        "\n",
        "\n",
        "Q2. Adjusted R-squared:\n",
        "\n",
        "Adjusted R-squared is a modified version of R-squared that takes into account the number of independent variables in the model. It penalizes the inclusion of unnecessary predictors, helping to prevent overfitting.\n",
        "Adjusted R-squared is calculated as: Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - p - 1)], where n is the number of data points and p is the number of predictors.\n",
        "It tends to be lower than R-squared when extra predictors are added unless those predictors significantly improve the model.\n",
        "\n",
        "\n",
        "Q3. When to Use Adjusted R-squared:\n",
        "\n",
        "Adjusted R-squared is more appropriate when you are comparing models with different numbers of predictors. It penalizes the addition of irrelevant variables.\n",
        "It helps you assess whether adding more predictors is worth the potential loss in model simplicity.\n",
        "\n",
        "\n",
        "Q4. RMSE, MSE, and MAE:\n",
        "\n",
        "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are common metrics used to evaluate regression models:\n",
        "RMSE is the square root of the average of the squared differences between actual and predicted values.\n",
        "MSE is the average of the squared differences between actual and predicted values.\n",
        "MAE is the average of the absolute differences between actual and predicted values.\n",
        "\n",
        "\n",
        "Q5. Advantages and Disadvantages of RMSE, MSE, and MAE:\n",
        "\n",
        "Advantages:\n",
        "\n",
        "RMSE and MSE give higher weight to larger errors, making them sensitive to outliers.\n",
        "MAE is more robust to outliers.\n",
        "All three metrics are easy to interpret.\n",
        "Disadvantages:\n",
        "\n",
        "RMSE and MSE penalize large errors more heavily, which might not always be desired.\n",
        "RMSE and MSE are in squared units, which can be less interpretable.\n",
        "MAE may not capture the impact of extreme errors effectively.\n",
        "\n",
        "\n",
        "Q6. Lasso Regularization:\n",
        "\n",
        "Lasso (Least Absolute Shrinkage and Selection Operator) regularization adds a penalty term to the linear regression loss function to encourage smaller coefficient values. It can force some coefficients to become exactly zero, effectively performing feature selection.\n",
        "It differs from Ridge regularization in that it tends to produce sparse models with only a subset of important features.\n",
        "\n",
        "\n",
        "Q7. Regularized Linear Models and Overfitting:\n",
        "\n",
        "Regularized linear models like Lasso and Ridge help prevent overfitting by adding penalty terms to the loss function. These penalties discourage excessively large coefficients, making the model less sensitive to noise and less likely to overfit.\n",
        "Example: In Lasso regression, if you have many irrelevant features, it might set some coefficients to zero, effectively removing those features from the model.\n",
        "\n",
        "\n",
        "Q8. Limitations of Regularized Linear Models:\n",
        "\n",
        "Regularization may not work well if there is no multicollinearity or if all features are genuinely important.\n",
        "The choice between Ridge and Lasso depends on the nature of the problem and the importance of feature selection.\n",
        "Regularization may not be suitable for non-linear relationships, requiring more complex models.\n",
        "\n",
        "\n",
        "Q9. Comparing Models with RMSE and MAE:\n",
        "\n",
        "In this case, Model B (MAE = 8) would be chosen as the better performer because MAE is less sensitive to outliers and provides a more balanced view of prediction errors.\n",
        "However, the choice of metric depends on the specific problem and the importance of different types of errors.\n",
        "\n",
        "\n",
        "Q10. Comparing Ridge and Lasso Regularization:\n",
        "- The choice between Ridge and Lasso regularization depends on the problem and the goals:\n",
        "- If you want to reduce overfitting while keeping all features, Ridge (with a smaller regularization parameter) might be preferable.\n",
        "- If you suspect that some features are irrelevant and can be eliminated, Lasso (with a larger regularization parameter) might be better.\n",
        "- The choice involves trade-offs, and it's crucial to tune the regularization parameters for optimal performance."
      ],
      "metadata": {
        "id": "T75iHX1EOOQr"
      }
    }
  ]
}