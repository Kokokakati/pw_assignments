{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
        "example of each.\n",
        "\n",
        "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
        "a given dataset?\n",
        "\n",
        "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
        "a real-world scenario.\n",
        "\n",
        "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
        "\n",
        "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
        "\n",
        "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
        "address this issue?\n",
        "\n",
        "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
        "\n",
        "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
        "regression? In what situations would you prefer to use polynomial regression?"
      ],
      "metadata": {
        "id": "_cKLebywrk3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Simple Linear Regression vs. Multiple Linear Regression:\n",
        "\n",
        "Simple Linear Regression: In simple linear regression, you have one independent variable (predictor) and one dependent variable (response). The relationship between them is modeled as a straight line, i.e., a linear equation of the form: Y = a + bX, where Y is the dependent variable, X is the independent variable, a is the intercept, and b is the slope.\n",
        "\n",
        "Example: Predicting a person's weight (Y) based on their height (X) using a simple linear regression model.\n",
        "Multiple Linear Regression: In multiple linear regression, you have more than one independent variable. The relationship is modeled as a linear equation, but with multiple predictors: Y = a + b1X1 + b2X2 + ... + bnXn, where Y is the dependent variable, X1, X2, ..., Xn are the independent variables, a is the intercept, and b1, b2, ..., bn are the slopes associated with each independent variable.\n",
        "\n",
        "Example: Predicting a house's price (Y) based on its size (X1), number of bedrooms (X2), and location (X3) using a multiple linear regression model.\n",
        "\n",
        "\n",
        "2. Assumptions of Linear Regression:\n",
        "\n",
        "Linearity: The relationship between independent and dependent variables is linear.\n",
        "Independence: Residuals (the differences between actual and predicted values) are independent of each other.\n",
        "Homoscedasticity: The variance of residuals is constant across all levels of the independent variables.\n",
        "Normality: Residuals are normally distributed.\n",
        "No multicollinearity: Independent variables are not highly correlated.\n",
        "You can check these assumptions through various methods like residual plots, normality tests, scatterplots, and correlation matrices.\n",
        "\n",
        "\n",
        "\n",
        "3. Interpreting Slope and Intercept:\n",
        "\n",
        "Slope (b): It represents the change in the dependent variable for a one-unit change in the independent variable, holding all other variables constant.\n",
        "\n",
        "Intercept (a): It is the value of the dependent variable when all independent variables are set to zero.\n",
        "\n",
        "Example: In a simple linear regression model predicting exam scores (Y) based on hours of study (X), the slope (b) might be 0.5, meaning that for each additional hour of study, the expected increase in exam score is 0.5 points. The intercept (a) might be 60, indicating that if a student studies for zero hours, their expected exam score is 60.\n",
        "\n",
        "\n",
        "\n",
        "4. Gradient Descent:\n",
        "\n",
        "Gradient descent is an optimization algorithm used in machine learning to minimize the loss function (a measure of error) of a model. It iteratively adjusts model parameters (weights) to find the minimum of the loss function.\n",
        "It works by computing the gradient of the loss function with respect to the model parameters and updating the parameters in the opposite direction of the gradient to reduce the loss.\n",
        "Gradient descent is used in training various machine learning models, including linear regression, neural networks, and support vector machines.\n",
        "\n",
        "\n",
        "\n",
        "5. Multiple Linear Regression:\n",
        "\n",
        "Multiple linear regression is an extension of simple linear regression that allows you to model the relationship between a dependent variable and multiple independent variables.\n",
        "The equation for multiple linear regression is: Y = a + b1X1 + b2X2 + ... + bnXn, where Y is the dependent variable, X1, X2, ..., Xn are the independent variables, a is the intercept, and b1, b2, ..., bn are the slopes associated with each independent variable.\n",
        "It captures more complex relationships by considering multiple predictors simultaneously.\n",
        "\n",
        "\n",
        "\n",
        "6. Multicollinearity in Multiple Linear Regression:\n",
        "\n",
        "Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated, making it difficult to distinguish their individual effects.\n",
        "You can detect multicollinearity using correlation matrices, scatterplots, or variance inflation factors (VIFs).\n",
        "To address multicollinearity, you can remove one of the correlated variables, combine them into a single variable, or use dimensionality reduction techniques like principal component analysis (PCA).\n",
        "\n",
        "\n",
        "\n",
        "7. Polynomial Regression:\n",
        "\n",
        "a)Polynomial regression is a type of regression analysis in which the relationship between the dependent variable and independent variable(s) is modeled as an nth-degree polynomial.\n",
        "b)Unlike linear regression, which assumes a linear relationship, polynomial regression can capture nonlinear patterns in data.\n",
        "c)The equation for polynomial regression is of the form: Y = a + b1X + b2X^2 + ... + bnx^n.\n",
        "\n",
        "\n",
        "\n",
        "8. Advantages and Disadvantages of Polynomial Regression:\n",
        "\n",
        "Advantages:\n",
        "\n",
        "a)Can model nonlinear relationships.\n",
        "b)Provides a better fit for certain types of data.\n",
        "c)Allows for more flexible modeling.\n",
        "Disadvantages:\n",
        "\n",
        "a)Can lead to overfitting if the degree of the polynomial is too high.\n",
        "b)Interpretability can be challenging with higher-degree polynomials.\n",
        "c)Extrapolation can be unreliable.\n",
        "Use polynomial regression when you believe there's a nonlinear relationship between variables, but be cautious about overfitting and select an appropriate polynomial degree based on data and validation techniques. Linear regression is preferable for simple, linear relationships."
      ],
      "metadata": {
        "id": "MSb5HsBDrolu"
      }
    }
  ]
}