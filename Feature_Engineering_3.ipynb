{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
        "application.\n",
        "\n",
        "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
        "Provide an example to illustrate its application.\n",
        "\n",
        "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
        "example to illustrate its application.\n",
        "\n",
        "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
        "Extraction? Provide an example to illustrate this concept.\n",
        "\n",
        "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
        "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
        "preprocess the data.\n",
        "\n",
        "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
        "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
        "dimensionality of the dataset.\n",
        "\n",
        "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
        "values to a range of -1 to 1.\n",
        "\n",
        "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform"
      ],
      "metadata": {
        "id": "F4NtpbnunuE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Min-Max scaling is a data preprocessing technique used to scale the features of a dataset to a specific range, usually between 0 and 1. It is achieved by subtracting the minimum value of each feature and then dividing by the range (the difference between the maximum and minimum values). Min-Max scaling is used to ensure that all features have the same scale, which can be important for some machine learning algorithms that are sensitive to the scale of the input data.\n",
        "\n",
        "Example of Min-Max scaling:\n",
        "Suppose we have a dataset with a feature \"Age\" ranging from 20 to 60 years. We want to scale this feature to the range of 0 to 1 using Min-Max scaling."
      ],
      "metadata": {
        "id": "MhDnpo5nnyma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Original Age values: [20, 30, 40, 50, 60]\n",
        "Min-Max scaling formula: X_scaled = (X - X_min) / (X_max - X_min)\n",
        "\n",
        "Scaled Age values: [(20-20)/(60-20), (30-20)/(60-20), (40-20)/(60-20), (50-20)/(60-20), (60-20)/(60-20)]\n",
        "Scaled Age values: [0.0, 0.1667, 0.3333, 0.5000, 1.0]\n"
      ],
      "metadata": {
        "id": "3nTps5bLn_9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: The Unit Vector technique in feature scaling, also known as L2 normalization or feature normalization, scales the feature vectors to have a Euclidean norm (magnitude) of 1. It differs from Min-Max scaling because it does not bound the features to a specific range but rather ensures that the vector representation of each data point has a consistent magnitude.\n",
        "\n",
        "Example of Unit Vector technique:\n",
        "Suppose we have a dataset with two features, \"Height\" and \"Weight.\" We want to apply Unit Vector scaling to each data point."
      ],
      "metadata": {
        "id": "Y_CAzdvKn5Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Original data points: [(150, 50), (170, 60), (160, 55)]\n",
        "Unit Vector scaling formula: X_normalized = X / ||X|| (where ||X|| is the Euclidean norm of X)\n",
        "\n",
        "Normalized data points: [ (150/√(150^2 + 50^2), 50/√(150^2 + 50^2)),\n",
        "                          (170/√(170^2 + 60^2), 60/√(170^2 + 60^2)),\n",
        "                          (160/√(160^2 + 55^2), 55/√(160^2 + 55^2)) ]\n",
        "Normalized data points: [(0.968, 0.252), (0.970, 0.244), (0.970, 0.244)]\n"
      ],
      "metadata": {
        "id": "sJ_Hk-o0n7d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while preserving its essential features. It achieves this by finding the principal components, which are orthogonal directions that capture the maximum variance in the data. By selecting only a subset of the principal components, we can reduce the dimensionality of the dataset while retaining most of its variability.\n",
        "\n",
        "Example of PCA:\n",
        "Suppose we have a dataset with two features, \"Height\" and \"Weight,\" and we want to reduce it to one dimension using PCA."
      ],
      "metadata": {
        "id": "RMSSSBJxn9WZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Original data points: [(150, 50), (170, 60), (160, 55)]\n",
        "\n",
        "Step 1: Standardize the data (mean = 0, variance = 1)\n",
        "Step 2: Compute the covariance matrix\n",
        "Step 3: Compute the eigenvectors and eigenvalues of the covariance matrix\n",
        "Step 4: Sort the eigenvectors based on their eigenvalues (descending order)\n",
        "Step 5: Choose the top principal component (eigenvector with the largest eigenvalue)\n",
        "\n",
        "Resulting reduced data points: [0.707 * (150, 50), 0.707 * (170, 60), 0.707 * (160, 55)]\n",
        "Reduced data points: [(106.1, 35.4), (119.9, 40.1), (113.1, 37.8)]\n"
      ],
      "metadata": {
        "id": "M0sQ-BB6oFRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: PCA can be used for Feature Extraction by considering the principal components as new features that capture the most significant information from the original features. Instead of using all the original features, we can choose a reduced set of principal components that explains a high percentage of the variance in the data. This not only reduces the dimensionality of the dataset but also helps remove noise and redundant information.\n",
        "\n",
        "Example of PCA for Feature Extraction:\n",
        "Suppose we have a dataset with three highly correlated features: \"Feature1,\" \"Feature2,\" and \"Feature3.\" We want to use PCA to create two new features that capture most of the variance in the data."
      ],
      "metadata": {
        "id": "g8p8EhzxoHWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Original data points: [(x1, y1, z1), (x2, y2, z2), (x3, y3, z3), ...]\n",
        "\n",
        "Step 1: Standardize the data (mean = 0, variance = 1)\n",
        "Step 2: Compute the covariance matrix\n",
        "Step 3: Compute the eigenvectors and eigenvalues of the covariance matrix\n",
        "Step 4: Sort the eigenvectors based on their eigenvalues (descending order)\n",
        "Step 5: Choose the top two principal components (eigenvectors with the largest eigenvalues)\n",
        "\n",
        "Resulting reduced data points: [ (x1', y1'), (x2', y2'), (x3', y3'), ... ]"
      ],
      "metadata": {
        "id": "gG1W9LkHoNzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: In the food delivery recommendation system project, Min-Max scaling can be used to preprocess the data to ensure that all features, such as price, rating, and delivery time, are on the same scale. This is important because different features may have different measurement units and scales. Min-Max scaling will bring all the features within the range of 0 to 1, making them comparable and avoiding dominance of one feature over others in the recommendation model.\n"
      ],
      "metadata": {
        "id": "Dn_pv9E2oKmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6: In the stock price prediction project, PCA can be used to reduce the dimensionality of the dataset, which typically contains many features related to company financial data and market trends. By reducing the dimensionality, we can focus on the most important patterns and variability in the data, which can lead to more efficient and accurate stock price prediction models."
      ],
      "metadata": {
        "id": "pRrv1AUooTNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7: Performing Min-Max scaling to transform the values to a range of -1 to 1 for the dataset [1, 5, 10, 15, 20]:"
      ],
      "metadata": {
        "id": "1Ds0fJiroVvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Min-Max scaling formula: X_scaled = 2 * (X - X_min) / (X_max - X_min) - 1\n",
        "\n",
        "Scaled values: [2 * (1 - 1) / (20 - 1) - 1,\n",
        "                2 * (5 - 1) / (20 - 1) - 1,\n",
        "                2 * (10 - 1) / (20 - 1) - 1,\n",
        "                2 * (15 - 1) / (20 - 1) - 1,\n",
        "                2 * (20 - 1) / (20 - 1) - 1]\n",
        "\n",
        "Scaled values: [-1, -0.5, 0.0, 0.5, 1.0]\n"
      ],
      "metadata": {
        "id": "DwxziVQPoXTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8: For the dataset containing features [height, weight, age, gender, blood pressure], performing PCA involves the following steps:\n",
        "\n",
        "Standardize the data (mean = 0, variance = 1) to ensure all features have the same scale.\n",
        "Compute the covariance matrix of the standardized data.\n",
        "\n",
        "Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
        "\n",
        "Sort the eigenvectors based on their eigenvalues in descending order.\n",
        "\n",
        "Choose the top k eigenvectors corresponding to the k highest eigenvalues to reduce the dataset to k dimensions.\n",
        "\n",
        "The resulting reduced dataset will have k dimensions, representing the most important patterns and variability in the original data. The selected k features will be the new set of features used for predicting the house price."
      ],
      "metadata": {
        "id": "ynRUVi0zobfj"
      }
    }
  ]
}