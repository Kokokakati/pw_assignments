{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the KNN algorithm?\n",
        "\n",
        "Q2. How do you choose the value of K in KNN?\n",
        "\n",
        "Q3. What is the difference between KNN classifier and KNN regressor?\n",
        "\n",
        "Q4. How do you measure the performance of KNN?\n",
        "\n",
        "Q5. What is the curse of dimensionality in KNN?\n",
        "\n",
        "Q6. How do you handle missing values in KNN?\n",
        "\n",
        "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
        "which type of problem?\n",
        "\n",
        "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
        "and how can these be addressed?\n",
        "\n",
        "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
        "\n",
        "Q10. What is the role of feature scaling in KNN?"
      ],
      "metadata": {
        "id": "aoLgmrZs9aYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. It works based on the principle of proximity, where it classifies or predicts the value of a data point by considering the majority class or the average of the K-nearest data points in the training dataset.\n",
        "\n",
        "Q2. Choosing the value of K in KNN depends on the dataset and the problem at hand. A small K (e.g., 1 or 3) makes the model sensitive to noise but might capture local patterns well. A larger K (e.g., 10 or 20) provides a smoother decision boundary but may oversimplify the model. The optimal K value is often determined using techniques like cross-validation or grid search.\n",
        "\n",
        "Q3. The main difference between a KNN classifier and a KNN regressor is in their output:\n",
        "\n",
        "KNN Classifier: Predicts the class label of a data point based on the majority class among its K-nearest neighbors.\n",
        "KNN Regressor: Predicts the numerical value of a data point based on the average (or other aggregation) of the values among its K-nearest neighbors.\n",
        "Q4. The performance of KNN can be measured using various metrics, including accuracy, precision, recall, F1-score, mean squared error (MSE), and R-squared (for regression). The choice of metric depends on whether you are performing classification or regression.\n",
        "\n",
        "Q5. The curse of dimensionality in KNN refers to the challenge that arises when dealing with high-dimensional data. As the number of dimensions (features) increases, the distance between data points becomes less meaningful, and the algorithm may struggle to find relevant neighbors. This can lead to a loss of accuracy and increased computational complexity. Dimensionality reduction techniques or feature selection can help address this issue.\n",
        "\n",
        "Q6. Handling missing values in KNN can be done by either imputing missing values before applying KNN or by modifying the distance metric to handle missing values gracefully. Common approaches include replacing missing values with the mean or median of the feature or using more advanced imputation techniques like k-Nearest Neighbors Imputation.\n",
        "\n",
        "Q7. The choice between KNN classification and regression depends on the nature of the problem:\n",
        "\n",
        "KNN Classifier is suitable for problems where the output is categorical (e.g., class labels) and involves classification tasks.\n",
        "KNN Regressor is appropriate for problems with numerical target values and regression tasks.\n",
        "Q8. Strengths of KNN:\n",
        "\n",
        "Simplicity and ease of implementation.\n",
        "Suitable for both classification and regression tasks.\n",
        "Non-parametric and can handle complex decision boundaries.\n",
        "Weaknesses of KNN:\n",
        "\n",
        "Sensitivity to the choice of K.\n",
        "Computationally expensive for large datasets.\n",
        "Performance may degrade in high-dimensional spaces (curse of dimensionality).\n",
        "These weaknesses can be addressed through hyperparameter tuning, dimensionality reduction, and efficient indexing structures.\n",
        "\n",
        "Q9. The difference between Euclidean distance and Manhattan distance in KNN is in the way they measure the distance between data points:\n",
        "\n",
        "Euclidean Distance: Measures the straight-line (shortest) distance between two points in Euclidean space, often used for continuous data.\n",
        "Manhattan Distance: Measures the distance as the sum of absolute differences between corresponding coordinates of the two points, often used for data with discrete or categorical features.\n",
        "Q10. Feature scaling in KNN is essential to ensure that all features contribute equally to distance calculations. Without scaling, features with larger ranges can dominate the distance metric. Common scaling techniques include z-score normalization (standardization) and min-max scaling (normalization) to bring features within a similar range."
      ],
      "metadata": {
        "id": "b3TLgK9Y9VZ1"
      }
    }
  ]
}