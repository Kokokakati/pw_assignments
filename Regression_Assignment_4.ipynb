{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
        "\n",
        "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
        "\n",
        "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
        "\n",
        "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
        "model's performance?\n",
        "\n",
        "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
        "\n",
        "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
        "\n",
        "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
        "\n",
        "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
      ],
      "metadata": {
        "id": "xjpRGtC1S4V7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Lasso Regression and Its Differences:\n",
        "\n",
        "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that adds an L1 regularization term to the linear regression loss function.\n",
        "Unlike ordinary least squares (OLS) regression, Lasso introduces a penalty on the absolute values of the coefficients, encouraging some coefficients to become exactly zero, effectively performing feature selection.\n",
        "Lasso differs from Ridge Regression, which uses an L2 regularization term to shrink coefficient values without forcing them to be exactly zero.\n",
        "\n",
        "\n",
        "Q2. Advantage of Lasso Regression in Feature Selection:\n",
        "\n",
        "The main advantage of Lasso Regression in feature selection is its ability to automatically select a subset of the most important features by setting the coefficients of less important features to zero.\n",
        "This feature selection property can help simplify models, improve interpretability, and potentially reduce overfitting.\n",
        "\n",
        "\n",
        "Q3. Interpreting Coefficients in Lasso Regression:\n",
        "\n",
        "Interpreting coefficients in Lasso Regression is similar to ordinary linear regression.\n",
        "A positive coefficient means that an increase in the corresponding independent variable increases the dependent variable's predicted value, and vice versa for a negative coefficient.\n",
        "The difference lies in the magnitude of coefficients. Smaller coefficients in Lasso indicate that the corresponding features have been downweighted or effectively removed.\n",
        "\n",
        "\n",
        "Q4. Tuning Parameters in Lasso Regression:\n",
        "\n",
        "The primary tuning parameter in Lasso Regression is λ (lambda), which controls the strength of the L1 regularization penalty.\n",
        "As λ increases, the penalty on the absolute values of coefficients becomes stronger, leading to more coefficients being pushed toward zero.\n",
        "You can adjust λ to control the balance between model simplicity and fit to the data.\n",
        "\n",
        "\n",
        "Q5. Lasso Regression for Non-Linear Problems:\n",
        "\n",
        "Lasso Regression, in its standard form, is a linear regression technique and is not designed for non-linear problems.\n",
        "However, it can be used in combination with feature engineering techniques like polynomial feature expansion to capture non-linear relationships between variables.\n",
        "\n",
        "\n",
        "Q6. Difference Between Ridge and Lasso Regression:\n",
        "\n",
        "Ridge Regression uses L2 regularization, which adds a penalty term proportional to the square of the coefficients, encouraging smaller but non-zero coefficients.\n",
        "Lasso Regression uses L1 regularization, which adds a penalty term proportional to the absolute values of the coefficients, effectively forcing some coefficients to become exactly zero. This results in feature selection.\n",
        "\n",
        "\n",
        "Q7. Lasso Regression and Multicollinearity:\n",
        "\n",
        "Lasso Regression can handle multicollinearity (high correlation between independent variables) by selecting one of the correlated variables and setting the coefficients of the others to zero.\n",
        "It effectively performs feature selection, choosing one representative from a group of highly correlated variables.\n",
        "\n",
        "\n",
        "Q8. Choosing the Optimal Regularization Parameter (λ) in Lasso Regression:\n",
        "\n",
        "The optimal value of λ can be chosen through techniques like cross-validation. You fit Lasso Regression models with different λ values on training data and select the one that minimizes prediction error (e.g., mean squared error) on a validation set.\n",
        "Cross-validation helps you find the right balance between model complexity and fit to the data, preventing overfitting."
      ],
      "metadata": {
        "id": "lAYnUTrlS_DT"
      }
    }
  ]
}