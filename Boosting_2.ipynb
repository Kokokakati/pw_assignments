{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Gradient Boosting Regression?\n",
        "\n",
        "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
        "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
        "performance using metrics such as mean squared error and R-squared.\n",
        "\n",
        "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
        "optimise the performance of the model. Use grid search or random search to find the best\n",
        "hyperparameters\n",
        "\n",
        "Q4. What is a weak learner in Gradient Boosting?\n",
        "\n",
        "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
        "\n",
        "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
        "\n",
        "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
        "algorithm?"
      ],
      "metadata": {
        "id": "2xZ6Unv1bbxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Gradient Boosting Regression is a machine learning technique used for regression tasks. It is an ensemble method that combines the predictions of multiple weak learners (typically decision trees) in a sequential manner. Each weak learner is trained to predict the residuals (errors) of the previous learners, and the final prediction is obtained by summing up the predictions of all learners."
      ],
      "metadata": {
        "id": "ROI444Nubi5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Here's a simple implementation of a gradient boosting algorithm from scratch using Python and NumPy:"
      ],
      "metadata": {
        "id": "KPDdzXAgbmvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize predictions with the mean value of y\n",
        "        predictions = np.full_like(y, np.mean(y))\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Compute residuals\n",
        "            residuals = y - predictions\n",
        "\n",
        "            # Fit a decision tree to residuals\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "\n",
        "            # Update predictions using the learning rate and predictions from the tree\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "\n",
        "            # Store the trained tree\n",
        "            self.models.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Initialize predictions with the mean value of y\n",
        "        predictions = np.full(X.shape[0], np.mean(y))\n",
        "\n",
        "        # Make predictions using each tree in the ensemble\n",
        "        for tree in self.models:\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Example usage:\n",
        "# Define X and y\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 3, 4, 5, 6])\n",
        "\n",
        "# Initialize and fit the model\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "x9rJ_I5EbpRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. To experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimize the performance of the model, you can perform grid search or random search. Grid search exhaustively searches through a specified parameter grid, while random search samples hyperparameters randomly. After training the model with different hyperparameters, you can evaluate each model's performance using metrics such as mean squared error and R-squared, and select the hyperparameters that yield the best performance.\n",
        "\n",
        "\n",
        "\n",
        "Q4. A weak learner in Gradient Boosting is a simple model that performs slightly better than random guessing. Typically, decision trees with shallow depths are used as weak learners in Gradient Boosting.\n",
        "\n",
        "\n",
        "\n",
        "Q5. The intuition behind the Gradient Boosting algorithm is to sequentially train a series of weak learners (usually decision trees) where each subsequent learner corrects the errors made by the previous ones. This is achieved by fitting each learner to the residuals (the differences between the actual values and the predicted values) of the previous model.\n",
        "\n",
        "\n",
        "\n",
        "Q6. The Gradient Boosting algorithm builds an ensemble of weak learners by sequentially adding models to the ensemble. Each new weak learner focuses on minimizing the residuals (errors) of the ensemble's predictions made by the previous models. The final prediction is obtained by summing up the predictions of all weak learners.\n",
        "\n",
        "\n",
        "\n",
        "Q7. The steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm include:\n",
        "\n",
        "Initialize the ensemble with a simple model (e.g., a single decision tree).\n",
        "Compute the residuals (errors) between the actual values and the predictions of the ensemble.\n",
        "\n",
        "Fit a weak learner (e.g., decision tree) to the residuals.\n",
        "\n",
        "Update the ensemble's predictions by adding the predictions of the new weak learner, scaled by a learning rate.\n",
        "\n",
        "Repeat steps 2-4 until a predefined number of iterations or until the residuals cannot be further reduced."
      ],
      "metadata": {
        "id": "RgyMsONxbqwT"
      }
    }
  ]
}