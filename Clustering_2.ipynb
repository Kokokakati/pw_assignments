{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is hierarchical clustering, and how is it different from other clustering techniques?\n",
        "\n",
        "Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief.\n",
        "\n",
        "Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the\n",
        "common distance metrics used?\n",
        "\n",
        "Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some\n",
        "common methods used for this purpose?\n",
        "\n",
        "Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?\n",
        "\n",
        "Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the\n",
        "distance metrics different for each type of data?\n",
        "\n",
        "Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?"
      ],
      "metadata": {
        "id": "QOK8r86IjV-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Hierarchical clustering is a method of cluster analysis that builds a hierarchy of clusters. Unlike partitioning methods like K-means, hierarchical clustering doesn't require specifying the number of clusters beforehand. It is different in that it produces a tree-like structure of nested clusters, where clusters at higher levels of the tree contain subclusters from lower levels.\n",
        "\n",
        "\n",
        "Q2. The two main types of hierarchical clustering algorithms are:\n",
        "\n",
        "a. Agglomerative Hierarchical Clustering: It starts with each data point as a single cluster and recursively merges the closest pairs of clusters until there's only one cluster containing all data points. It uses a \"bottom-up\" approach.\n",
        "\n",
        "b. Divisive Hierarchical Clustering: It begins with all data points in a single cluster and recursively divides clusters into smaller clusters until each data point forms its own cluster. It uses a \"top-down\" approach.\n",
        "\n",
        "\n",
        "Q3. The distance between two clusters in hierarchical clustering is determined using various distance metrics, such as:\n",
        "\n",
        "Single Linkage (Minimum): The distance between two clusters is the shortest distance between any pair of data points in the two clusters.\n",
        "Complete Linkage (Maximum): The distance is the longest distance between any pair of data points in the two clusters.\n",
        "Average Linkage: The distance is the average of pairwise distances between data points in the two clusters.\n",
        "Centroid Linkage: The distance is between the centroids (means) of the two clusters.\n",
        "Ward's Linkage: Minimizes the increase in total within-cluster variance when merging clusters.\n",
        "\n",
        "\n",
        "Q4. Determining the optimal number of clusters in hierarchical clustering can be done using methods like:\n",
        "\n",
        "Dendrogram: Visual inspection of the dendrogram to identify natural cut-off points.\n",
        "Inconsistency Index: Measures the height at which a cluster is merged relative to the average height of its children.\n",
        "Cophenetic Correlation Coefficient: Measures how faithfully the dendrogram preserves the pairwise distances between data points.\n",
        "Gap Statistics: Compares the clustering quality to that of a random clustering.\n",
        "\n",
        "\n",
        "Q5. Dendrograms are tree-like diagrams that represent the hierarchical structure of clusters. They are useful in analyzing the results of hierarchical clustering by showing how clusters are formed and at what level of similarity they are merged. Dendrograms help identify natural clusters and decide on an appropriate number of clusters.\n",
        "\n",
        "\n",
        "Q6. Yes, hierarchical clustering can be used for both numerical and categorical data. The choice of distance metric depends on the data type:\n",
        "\n",
        "For numerical data, common distance metrics include Euclidean distance, Manhattan distance, and others.\n",
        "For categorical data, you can use metrics like the Jaccard distance or the Hamming distance, which are designed for binary data or categorical data.\n",
        "\n",
        "\n",
        "Q7. Hierarchical clustering can be used to identify outliers or anomalies by observing the structure of the dendrogram. Outliers often appear as individual branches or clusters with very few data points and greater distances from other clusters. You can set a threshold distance to cut the dendrogram, forming clusters, and any data points remaining as singletons or in small clusters can be considered outliers. Additionally, you can use statistical methods to identify outliers within individual clusters formed by hierarchical clustering."
      ],
      "metadata": {
        "id": "J2g2FmPujZoR"
      }
    }
  ]
}